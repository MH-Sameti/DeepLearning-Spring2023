From f8718cf7412f23bd7ac2730ee58484c70a557001 Mon Sep 17 00:00:00 2001
From: "mh_sameti@yahoo.com" <mh_sameti@yahoo.com>
Date: Tue, 2 May 2023 18:24:00 +0000
Subject: [PATCH] #

---
 src/transformers/models/roberta/modeling_roberta.py | 13 ++++++++++++-
 1 file changed, 12 insertions(+), 1 deletion(-)

diff --git a/src/transformers/models/roberta/modeling_roberta.py b/src/transformers/models/roberta/modeling_roberta.py
index 75b728af6..3b34a517f 100644
--- a/src/transformers/models/roberta/modeling_roberta.py
+++ b/src/transformers/models/roberta/modeling_roberta.py
@@ -289,16 +289,21 @@ class RobertaSelfOutput(nn.Module):
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        self.Adapter = nn.Linear( config.hidden_size, config.hidden_size//2)
+        self.Adapter_act = nn.GELU()
+        self.Adapter2 = nn.Linear(config.hidden_size//2,  config.hidden_size)
         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.dropout = nn.Dropout(config.hidden_dropout_prob)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
         hidden_states = self.dropout(hidden_states)
+        hidden_states = self.Adapter(hidden_states)
+        hidden_states = self.Adapter_act(hidden_states)
+        hidden_states = self.Adapter2(hidden_states)
         hidden_states = self.LayerNorm(hidden_states + input_tensor)
         return hidden_states
 
-
 # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta
 class RobertaAttention(nn.Module):
     def __init__(self, config, position_embedding_type=None):
@@ -371,11 +376,17 @@ class RobertaOutput(nn.Module):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.Adapter = nn.Linear( config.hidden_size, config.hidden_size//2)
+        self.Adapter_act = nn.GELU()
+        self.Adapter2 = nn.Linear(config.hidden_size//2,  config.hidden_size)
         self.dropout = nn.Dropout(config.hidden_dropout_prob)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
         hidden_states = self.dropout(hidden_states)
+        hidden_states = self.Adapter(hidden_states)
+        hidden_states = self.Adapter_act(hidden_states)
+        hidden_states = self.Adapter2(hidden_states)
         hidden_states = self.LayerNorm(hidden_states + input_tensor)
         return hidden_states
 
-- 
2.25.1

